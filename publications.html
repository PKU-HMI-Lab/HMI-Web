<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0058)https://group.iiis.tsinghua.edu.cn/~maks/publications.html -->
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7">
  <title>HMI Lab | PKUCS</title>
  <meta name="keywords"
    content="Shanghang Zhang, 仉尚航, Peking University, Implanted Devices, Self-driving, Nonvolatile Processor">
  <meta name="description"
    content="Dr. Shanghang Zhang is a Tenure Track Assistant Professor at the Computer Science
    Department of Peking University. She has been the postdoc research fellow at Berkeley AI
    Research Lab (BAIR), EECS, UC Berkeley.">
  <link rel="icon" href="./index_files/logo_1.jpeg" type="image/x-icon">
  <link rel="shortcut icon" href="./index_files/logo_1.jpeg" type="image/x-icon">
  <link rel="stylesheet" href="./publications/pub.css">
  <script src="./publications/hm.js"></script>
  <script>
    var _hmt = _hmt || [];
    (function () {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d46d812474bd8517a73135c4ec1ceae5";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>

  <style type="text/css">
    <!--
    .list1 {
      text-decoration: none;
      color: #068f88;
      font-size: 14px;
      font-family: 'Montserrat', "SF Pro Text", "SF Pro Icons", "Helvetica Neue", "Helvetica", "Arial", sans-serif;
    }

    .list1 a {
      text-decoration: none;
      color: #535353;
      font-size: 14px;
      font-family: 'Montserrat', "SF Pro Text", "SF Pro Icons", "Helvetica Neue", "Helvetica", "Arial", sans-serif;
    }

    .list1 a:hover {
      text-decoration: none;
      color: #068f88;
      font-size: 14px;
      font-family: 'Montserrat', "SF Pro Text", "SF Pro Icons", "Helvetica Neue", "Helvetica", "Arial", sans-serif;
      border-bottom: 1px solid #068f88;
    }

    p {
      margin: 5px 0 5px 0;
      padding: 0;
    }

    div img {
      cursor: pointer;
      transition: all 0.6s;
    }

    div img:hover {
      transform: scale(1.05);
    }
    -->
	.book {
        font-weight: bold;
        color: green;
    }
	.conference {
        font-weight: bold;
        color: red;
    }
    .journal {
        font-weight: bold;
        color: blue;
    }
    .author-bold {
        font-weight: bold;
        text-decoration: underline;
    }
  </style>

</head>

<body>
	<!-- menu -->
</div>
<div class="top">
    
    <div class="topmenu">
      <div class="logo" style="margin-top: 30px;">
        <table width="85px" border="0" cellspacing="8" cellpadding="0">
          <tbody>
            <tr>
              <td align="center"><img src="./index_files/logo_1.jpeg" width="80" height="50"></td>
            </tr>
            <tr>
              <td align="center" style="font-size: 1.2em; color: #333; font-weight: bold;">HMI Lab</td>
            </tr>
          </tbody>
        </table>
      </div>
      <div class="menu" style="margin-top: 50px;">
        <div style="display: flex; justify-content: space-around; width: 100%; background-color: #fff; padding: 2px 0;">
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./index.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>首页</span>
              <span>Homepage</span>
            </a>
          </div>
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./leader.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>首席研究员</span>
              <span>Principal Investigator</span>
            </a>
          </div>
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./about.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>实验室概况</span>
              <span>Overview</span>
            </a>
          </div>
          <!--
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./people.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>人才队伍</span>
              <span>People</span>
            </a>
          </div>
          -->
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./research.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>科研方向</span>
              <span>Research Directions</span>
            </a>
          </div>
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./publications.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>论文发表</span>
              <span>Publications</span>
            </a>
          </div>
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./join.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>加入我们</span>
              <span>Join Us</span>
            </a>
          </div>
          <div style="padding: 10px 25px; border: 0px solid #ddd; border-radius: 5px; transition: background-color 0.3s; height: 40px; display: flex; align-items: center;">
            <a href="./news.html" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px; display: flex; flex-direction: column; align-items: center; line-height: 2;">
              <span>新闻热点</span>
              <span>News</span>
            </a>
          </div>
          <img src="./join/pku_.png" width="60" height="60" style="margin-left: 10px;">
        </div>
      </div>
    </div>
  </div>
<div class="ban"></div>
<!-- menuover -->

<style>
.menu div:hover {
  background-color: #e9ecef;
}
</style>

  <!-- listbanner -->
  <table width="100%" bgcolor="#94070A" border="0" cellspacing="0" cellpadding="0">
<tbody><tr><td align="center">
<table width="1200px" border="0" cellspacing="0" cellpadding="0">
<tbody><tr><td>
<div class="listitle">Representative Publications</div>
</td>
  </tr>
</tbody></table>
</td>
  </tr>
</tbody></table>
  <!-- content -->
  <table width="100%" bgcolor="#fbfbfb" border="0" cellspacing="0" cellpadding="0">
    <tbody>
      <tr>
        <td align="center">
          <table width="1000px" border="0" cellspacing="0" cellpadding="0">
            <tbody>
              <tr>
                <td>
                  <div class="con2">
					<!-- 1 -->
					<br><br>
					<h2 style="font-size:24px; color:#ee1212; font-weight:700; margin-bottom:20px;">Books</h2>
					<br>

					<p class="contc1">
						<span class="book">[Springer Nature, 2020 Jun 29]</span>
						“Deep Reinforcement Learning: Fundamentals, Research and Applications”, Springer Nature, 2020 Jun 29, (Electronic Edition 250,000 downloads; selectd to Annual High-Impact Publications in Computer Science by Chinese researchers).
						<br>
						H. Dong, Z. Ding, <span class="author-bold">S. Zhangs</span>, eds.
					</p>

					<br><br>
					<h2 style="font-size:24px; color:#ee1212; font-weight:700; margin-bottom:20px;">Journals & Conferences</h2>
					<br>

					<p class="contc1">
						<span class="conference">[CVPR 2025]</span>
						Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, <span class="author-bold">Shanghang Zhang</span>, Angjoo Kanazawa, Qianqian Wang, Segment Any Motion in Videos, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/nnanhuang/SegAnyMo">https://github.com/nnanhuang/SegAnyMo</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2025]</span>
						Jiajun Cao, Yuan Zhang, Tao Huang, Ming Lu, Qizhe Zhang, Ruichuan An, Ningning MA, <span class="author-bold">Shanghang Zhang</span>, MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/hey-cjj/MoVE-KD">https://github.com/hey-cjj/MoVE-KD</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2025]</span>
						Yueru Jia, Jiaming Liu, Sixiang Chen, Chenyang Gu, Zhilve Wang, Xiaoqi Li, Longzan Luo, Pengwei Wang, Renrui Zhang, Zhongyuan Wang, <span class="author-bold">Shanghang Zhang</span>, Lift3D Policy: Lifting 2D Foundation Models for Robust 3D Robotic Manipulation, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/PKU-HMI-Lab/LIFT3D.git">https://github.com/PKU-HMI-Lab/LIFT3D.git</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2025]</span>
						Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, <span class="author-bold">Shanghang Zhang</span>, RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/FlagOpen/RoboBrain">https://github.com/FlagOpen/RoboBrain</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2025]</span>
						Jinchang Xu, Shaokang Wang, Jintao Chen, Zhe Li, Peidong Jia, Fei Zhao, Guoqing Xiang, Zhijian Hao, <span class="author-bold">Shanghang Zhang</span>, Xiaodong Xie, Decouple Distortion from Perception: Region Adaptive Diffusion for Extreme-low Bitrate Perception Image Compression, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/xjc97/mridc.git">https://github.com/xjc97/mridc.git</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICLR 2025]</span>
						Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, <span class="author-bold">Shanghang Zhang</span>, Wenhan Luo, Qifeng Liu, Yike Guo, “Co3Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion”, ICLR 2025, Spotlight (Top 5.1%).
						<br>
						<b>Codes:</b> <a href="https://mattie-e.github.io/Co3/">https://mattie-e.github.io/Co3/</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICLR 2025]</span>
						Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, <span class="author-bold">Shanghang Zhang</span>, Hongsheng Li, “Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want”, ICLR 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/AFeng-x/Draw-and-Understand">https://github.com/AFeng-x/Draw-and-Understand</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICLR 2025]</span>
						Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, <span class="author-bold">Shanghang Zhang</span>, Peng Gao, Hongsheng Li, “MAVIS: Mathematical Visual Instruction Tuning”, ICLR 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/ZrrSkywalker/MAVIS">https://github.com/ZrrSkywalker/MAVIS</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2025]</span>
						Jianing Li, Hao Wang, Guxixi Gu Chenyang, Ming Lu, Wenzhao Zheng, LI DU, <span class="author-bold">Shanghang Zhang</span>, SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice Representation, ICRA 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/NorthSummer/SliceOcc.git">https://github.com/NorthSummer/SliceOcc.git</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2025]</span>
						High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior, Nan Huang, Ting Zhang, Yuhui Yuan, Dong Chen, <span class="author-bold">Shanghang Zhang</span>, ICRA 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/nnanhuang/Customize-it-3D">https://github.com/nnanhuang/Customize-it-3D</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2025]</span>
						DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments, Ma, Ji,Dai, Ryan,Mu, Yao,Wu, Pengying,Wang, Hao,Chi, Xiaowei,Fei, Yang,<span class="author-bold">Zhang, Shanghang</span>,Liu, Chang, ICRA 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/JiMa25/DOZE-Dataset">https://github.com/JiMa25/DOZE-Dataset</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[AAAI 2025]</span>
						Bowen Liu, Haoyang Li, Shuning Wang, Shuo Nie, <span class="author-bold">Shanghang Zhang</span>, Subgraph Aggregation for Out-of-Distribution Generalization on Graphs, AAAI 2025. 
						<br>
						<b>Codes:</b> <a href="https://github.com/Nanolbw/SuGAr">https://github.com/Nanolbw/SuGAr</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[AAAI 2025]</span>
						Senqiao Yang, Jiaming Liu, Renrui Zhang, Mingjie Pan, Ziyu Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Hongsheng Li, Yandong Guo, <span class="author-bold">Shanghang Zhang</span>, LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding, AAAI 2025.
						<br>
						<b>Codes:</b> <a href="https://huggingface.co/datasets/Senqiao/LiDAR-LLM-Nu-Caption">https://huggingface.co/datasets/Senqiao/LiDAR-LLM-Nu-Caption</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[AAAI 2025]</span>
						Yueru Jia, Yuhui Yuan, Aosong Cheng, Chuke Wang, Ji Li, Huizhu Jia, <span class="author-bold">Shanghang Zhang</span>, DesignEdit: Unify Spatial-Aware Image Editing via Training-free Inpainting with a Multi-Layered Latent Diffusion Framework, AAAI 2025.
						<br>
						<b>Codes:</b> <a href="https://github.com/design-edit/DesignEdit.git">https://github.com/design-edit/DesignEdit.git</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2024]</span>
						Yuan Zhang, Fei xiao, Tao Huang, Chun-Kai Fan, Hongyuan Dong, Jiawen Li, Jiacong Wang, Kuan Cheng, <span class="author-bold">Shanghang Zhang*</span>, Haoyuan Guo*, Unveiling the Tapestry of Consistency in Large Vision-Language Models, Advances in Neural Information Processing Systems (NeurIPS), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/foundation-multimodal-models/ConBench">https://github.com/foundation-multimodal-models/ConBench</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2024]</span>
						Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao Yang, Renrui Zhang, Yandong Guo, <span class="author-bold">Shanghang Zhang*</span>, RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation, Advances in Neural Information Processing Systems (NeurIPS), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/lmzpai/roboMamba">https://github.com/lmzpai/roboMamba</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2024]</span>
						Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, <span class="author-bold">Shanghang Zhang</span>, Wei Xue, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, Yike Guo, Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention, Advances in Neural Information Processing Systems (NeurIPS), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/pengHTYX/Era3D">https://github.com/pengHTYX/Era3D</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[EMNLP 2024]</span>
						Shitian Zhao, Renrui Zhang, Xu Luo, Yan Wang, <span class="author-bold">Shanghang Zhang</span>, Peng Gao, Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models, EMNLP 2024
						<br>
						<b>Codes:</b> <a href="https://github.com/zhaoshitian/Likelihood-Composition-Toolkit">https://github.com/zhaoshitian/Likelihood-Composition-Toolkit</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[EMNLP 2024]</span>
						Xinyan Chen, Jiaxin Ge, Tianjun Zhang, Jiaming Liu, <span class="author-bold">Shanghang Zhang</span>, Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training, EMNLP 2024
						<br>
						<b>Codes:</b> <a href="https://github.com/xinyan-cxy/IPR-RLDF">https://github.com/xinyan-cxy/IPR-RLDF</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ECCV 2024]</span>
						Yulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, Jiaming Liu, <span class="author-bold">Shanghang Zhang</span>, LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model, ECCV 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/llm-as-dataset-analyst/SSDLLM">https://github.com/llm-as-dataset-analyst/SSDLLM</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ECCV 2024]</span>
						Xiaobao Wei, Jiajun Cao, Yizhu Jin, Ming Lu, Guangyu Wang, <span class="author-bold">Shanghang Zhang</span>, I-MedSAM: Implicit Medical Image Segmentation with Segment Anything, ECCV 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/ucwxb/I-MedSAM">https://github.com/ucwxb/I-MedSAM</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ACM International Conference on Multimedia 2024]</span>
						Rongyu Zhang, Zefan Cai, Huanrui Yang, Zidong Liu, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Baobao Chang, Yuan Du, Li Du, <span class="author-bold">Shanghang Zhang*</span>. "VeCAF: Vision-language Collaborative Active Finetuning with Training Objective Awareness." In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 5451-5459. 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/RoyZry98/VeCAF-Pytorch">https://github.com/RoyZry98/VeCAF-Pytorch</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICML 2024]</span>
						Anthony Chen, Huanrui Yang, Yulu Gan, Denis A Gudovskiy, Zhen Dong, Haofan Wang, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, <span class="author-bold">Shanghang Zhang*</span>. Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting. Accepted by ICML 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/antonioo-c/Split-Ensemble">https://github.com/antonioo-c/Split-Ensemble</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICML 2024]</span>
						Yixiong Zou, <span class="author-bold">Shanghang Zhang</span>, Haichen Zhou, Yuhua Li and Ruixuan Li. Compositional Few-Shot Class-Incremental Learning. Accepted by ICML 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/Zoilsen/Comp-FSCIL">https://github.com/Zoilsen/Comp-FSCIL</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICML 2024]</span>
						Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, <span class="author-bold">Shanghang Zhang*</span>, Chang Liu*. VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model, Accepted by ICML 2024.
						<br>
						<b>Codes:</b> <a href="https://voro-nav.github.io/">https://voro-nav.github.io/</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[JBHI 2024]</span>
						Xingqun Qi, Zhuojie Wu, Wenxuan Zou, Min Ren, Yifan Gao, Muyi Sun, <span class="author-bold">Shanghang Zhang</span>, Caifeng Shan, and Zhenan Sun. "Exploring generalizable distillation for efficient medical image segmentation." IEEE Journal of Biomedical and Health Informatics (JBHI) (2024).
						<br>
						<b>Codes:</b> <a href="https://github.com/XingqunQi-lab/GKD-Framework">https://github.com/XingqunQi-lab/GKD-Framework</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2024]</span>
						Yuan Zhang, Tao Huang, Jiaming Liu, Tao Jiang, Kuan Cheng, <span class="author-bold">Shanghang Zhang*</span>, FreeKD: Knowledge Distillation via Semantic Frequency Prompt. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/Gumpest/FreeKD">https://github.com/Gumpest/FreeKD</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2024]</span>
						Jiaming Liu, Ran Xu. Senqiao Yang. Renrui Zhang. Qizhe Zhang, Zehui Chen, Yandong Guo, <span class="author-bold">Shanghang Zhang*</span>, Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/RanXu2000/continual-mae?tab=readme-ov-file">https://github.com/RanXu2000/continual-mae?tab=readme-ov-file</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2024]</span>
						Xiaobao Wei, Renrui Zhang, Jiarui Wu, Jiaming Liu, Ming Lu, Yandong Guo, <span class="author-bold">Shanghang Zhang*</span>, NTO3D: Neural Target Object 3D Reconstruction with Segment Anything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/ucwxb/NTO3D">https://github.com/ucwxb/NTO3D</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2024]</span>
						Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, <span class="author-bold">Shanghang Zhang</span>, Qifeng Liu, Yike Guo, Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024.
						<br>
						<b>Codes:</b> <a href="https://xingqunqi-lab.github.io/Emo-Transition-Gesture/">https://xingqunqi-lab.github.io/Emo-Transition-Gesture/</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2024]</span>
						Guanqun Wang, Jiaming Liu, Chenxuan Li, Yuan Zhang, Ma Junpeng, Xinyu Wei, Kevin Zhang, Maurice Chong, Renrui Zhang, Yijiang Liu, <span class="author-bold">Shanghang Zhang*</span>, Cloud-Device Collaborative Learning for Multimodal Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/KongoCat/Cloud-Device-Collaborative-Learning-for-Multimodal-Large-Language-Models">https://github.com/KongoCat/Cloud-Device-Collaborative-Learning-for-Multimodal-Large-Language-Models</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2024]</span>
						Zhi Zhang, Qizhe Zhang, Zijun Gao, Renrui Zhang, Ekaterina Shutova, Shiji Zhou, <span class="author-bold">Shanghang Zhang*</span>, Gradient-based Parameter Selection for Efficient Fine-Tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/FightingFighting/GPS">https://github.com/FightingFighting/GPS</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[Nature Methods 2024]</span>
						Zhou, Yu, Jiajun Cao, Justin Sonneck, Sweta Banerjee, Stefanie Dörr, Anika Grüneboom, Kristina Lorenz, <span class="author-bold">Shanghang Zhang</span>, and Jianxu Chen. "EfficientBioAI: making bioimaging AI models efficient in energy and latency." Nature Methods (2024), (Nature Series Journal).
						<br>
						<b>Codes:</b> <a href="https://github.com/MMV-Lab/EfficientBioAI">https://github.com/MMV-Lab/EfficientBioAI</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2024]</span>
						Jiaming Liu, Qizhe Zhang, Xiaoqi Li, Jianing Li, Guanqun Wang, Ming Lu, Tiejun Huang, <span class="author-bold">Shanghang Zhang*</span>, Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer, International Conference on Robotics and Automation (ICRA), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/Theia-4869/BiCross">https://github.com/Theia-4869/BiCross</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2024]</span>
						Jiaming Liu, Rongyu Zhang, Xiaoqi Li, Xiaowei Chi, Zehui Chen, Ming Lu, Yandong Guo, <span class="author-bold">Shanghang Zhang*</span>, Multi-geometric Space Alignments for Domain Adaptive Multi-view 3D Object Detection, International Conference on Robotics and Automation (ICRA), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/RoyZry98/BEVUDA-Pytorch">https://github.com/RoyZry98/BEVUDA-Pytorch</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2024]</span>
						Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Bing Wang, Hongwei Xie, Li Liu, <span class="author-bold">Shanghang Zhang*</span>, RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision, International Conference on Robotics and Automation (ICRA), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/pmj110119/RenderOcc">https://github.com/pmj110119/RenderOcc</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICRA 2024]</span>
						Jiayi Ni, Senqiao Yang, Jiaming Liu, Xiaoqi Li, Wenyu Jiao, Ran Xu, Zehui Chen, Yi Liu, <span class="author-bold">Shanghang Zhang*</span>, Distribution-Aware Continual Test Time Adaptation for Semantic Segmentation, International Conference on Robotics and Automation (ICRA), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/RochelleNi/DAT?tab=readme-ov-file">https://github.com/RochelleNi/DAT?tab=readme-ov-file</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICLR 2024]</span>
						Jiaming Liu, Senqiao Yang, Peidong Jia, Renrui Zhang, Ming Lu, Yandong Guo, Wei Xue, <span class="author-bold">Shanghang Zhang*</span>, ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation, The Twelfth International Conference on Learning Representations (ICLR), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/Yangsenqiao/vida">https://github.com/Yangsenqiao/vida</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICLR 2024]</span>
						Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, <span class="author-bold">Shanghang Zhang</span>, Jie Fu, Zhiyuan Liu, ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate, The Twelfth International Conference on Learning Representations (ICLR), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/thunlp/ChatEval">https://github.com/thunlp/ChatEval</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[AAAI 2024]</span>
						Yang S, Wu J, Liu J, Li X, Zhang Q, Pan M, Gan Y, Chen Z, <span class="author-bold">Zhang S</span>, Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction, Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/Anonymous-012/SVDP">https://github.com/Anonymous-012/SVDP</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[AAAI 2024]</span>
						Zhang R, Luo Y, Liu J, Yang H, Dong Z, Gudovskiy D, Okuno T, Nakata Y, Keutzer K, Du Y, <span class="author-bold">Zhang S</span>, Efficient Deweahter Mixture-of-Experts with Uncertainty-Aware Feature-wise Linear Modulation, Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI), 2024.
						<br>
						<b>Codes:</b> <a href="https://github.com/RoyZry98/MoFME-Pytorch">https://github.com/RoyZry98/MoFME-Pytorch</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2023]</span>
						Zhou Q, Li W, Jiang L, Wang G, Zhou G, <span class="author-bold">Zhang S</span>, Zhao H. PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection. Advances in Neural Information Processing Systems (NeurIPS), 2023.
						<br>
						<b>Codes:</b> <a href="https://github.com/EricLee0224/PAD">https://github.com/EricLee0224/PAD</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[IEEE Transactions on Intelligent Vehicles 2023]</span>
						Li J, Lu M, Liu J, Guo Y, Du Y, Du L, <span class="author-bold">Zhang S*</span>. BEV-LGKD: A Unified LiDAR-Guided Knowledge Distillation Framework for Multi-View BEV 3D Object Detection. IEEE Transactions on Intelligent Vehicles. 2023 Sep 26.
						<br>
						<b>Codes:</b> <a href="https://github.com/NorthSummer/LGKD.git">https://github.com/NorthSummer/LGKD.git</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICCV 2023]</span>
						Li X, Liu Y, Lian L, Yang H, Dong Z, Kang D, <span class="author-bold">Zhang S</span>, Keutzer K. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2023 (pp. 17535-17545).
						<br>
						<b>Codes:</b> <a href="https://github.com/Xiuyu-Li/q-diffusion">https://github.com/Xiuyu-Li/q-diffusion</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICCV 2023]</span>
						Zhu X, Zhang R, He B, Guo Z, Zeng Z, Qin Z, <span class="author-bold">Zhang S</span>, Gao P. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2023 (pp. 2639-2650).
						<br>
						<b>Codes:</b> <a href="https://github.com/yangyangyang127/PointCLIP_V2">https://github.com/yangyangyang127/PointCLIP_V2</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[DAC 2023]</span>
						Xiao L, Yang H, Dong Z, Keutzer K, Du L, <span class="author-bold">Zhang S*</span>. Csq: Growing mixed-precision quantization scheme with bi-level continuous sparsification. In2023 60th ACM/IEEE Design Automation Conference (DAC) 2023 Jul 9 (pp. 1-6). IEEE.
						<br>
						<b>Codes:</b> <a href="https://github.com/lawsonX/CSQ">https://github.com/lawsonX/CSQ</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2023]</span>
						Chi X, Liu J, Lu M, Zhang R, Wang Z, Guo Y, <span class="author-bold">Zhang S*</span>. BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023 (pp. 17461-17470).
						<br>
						<b>Codes:</b> <a href="https://github.com/litwellchi/BEV-SAN.git">https://github.com/litwellchi/BEV-SAN.git</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2023]</span>
						Lu Y, Xu C, Wei X, Xie X, Tomizuka M, Keutzer K, <span class="author-bold">Zhang S*</span>. Open-vocabulary point-cloud object detection without 3d annotation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023 (pp. 1190-1199).
						<br>
						<b>Codes:</b> <a href="https://github.com/lyhdet/OV-3DET">https://github.com/lyhdet/OV-3DET</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2023]</span>
						Chen A, Zhang K, Zhang R, Wang Z, Lu Y, Guo Y, <span class="author-bold">Zhang S*</span>. Pimae: Point cloud and image interactive masked autoencoders for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023 (pp. 5291-5301).
						<br>
						<b>Codes:</b> <a href="https://github.com/antonioo-c/PiMAE">https://github.com/antonioo-c/PiMAE</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2023]</span>
						Liu Y, Yang H, Dong Z, Keutzer K, Du L, <span class="author-bold">Zhang S*</span>. NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023 (pp. 20321-20330).
						<br>
						<b>Codes:</b> <a href="https://github.com/kriskrisliu/NoisyQuant">https://github.com/kriskrisliu/NoisyQuant</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2023]</span>
						Gu J, Wang K, Luo H, Chen C, Jiang W, Fang Y, <span class="author-bold">Zhang S</span>, You Y, Zhao J. MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023 (pp. 19243-19253).
						<br>
						<b>Codes:</b> <a href="https://github.com/vimar-gu/MSINet">https://github.com/vimar-gu/MSINet</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2023]</span>
						Ma Y, Li H, Zhang Z, Guo J, <span class="author-bold">Zhang S</span>, Gong R, Liu X. Annealing-Based Label-Transfer Learning for Open World Object Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023 (pp. 11454-11463).
						<br>
						<b>Codes:</b> <a href="https://github.com/DIG-Beihang/ALLOW">https://github.com/DIG-Beihang/ALLOW</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NOSSDAV 2023]</span>
						Zhang R, Du L, Liu J, Song C, Wang F, Li X, Lu M, Guo Y, <span class="author-bold">Zhang S.*</span>. RepCaM: Re-parameterization Content-aware Modulation for Neural Video Delivery. In Proceedings of the 33rd Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV, CCF B), 2023 Jun 7 (pp. 1-7).
						<br>
						<b>Codes:</b> <a href="https://github.com/RoyZry98/RepCaM-Pytorch">https://github.com/RoyZry98/RepCaM-Pytorch</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2022]</span>
						Wei X, Zhang Y, Zhang X, Gong R, <span class="author-bold">Zhang S</span>, Zhang Q, Yu F, Liu X. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems (NeurIPS). 2022 Dec 6;35:17402-14.
						<br>
						<b>Codes:</b> <a href="https://github.com/wimh966/outlier_suppression?tab=readme-ov-file">https://github.com/wimh966/outlier_suppression?tab=readme-ov-file</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2022]</span>
						Zhou H, Xiao S, <span class="author-bold">Zhang S</span>, Peng J, Zhang S, Li J. Jump Self-attention: Capturing High-order Statistics in Transformers. Advances in Neural Information Processing Systems (NeurIPS). 2022 Dec 6;35:17899-910.
						<br>
						<b>Codes:</b> <a href="https://github.com/zhouhaoyi/JAT2022">https://github.com/zhouhaoyi/JAT2022</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[NeurIPS 2022]</span>
						Zou Y, <span class="author-bold">Zhang S</span>, Li Y, Li R. Margin-based few-shot class-incremental learning with class-level overfitting mitigation. Advances in neural information processing systems (NeurIPS). 2022 Dec 6;35:27267-79.
						<br>
						<b>Codes:</b> <a href="https://github.com/Zoilsen/CLOM">https://github.com/Zoilsen/CLOM</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ECCV 2022]</span>
						J. Yu, J. Liu, X.Wei, H. Zhou, Y. Nakata, D. Gudovskiy, T. Okuno, J. Li, K. Keutzer, <span class="author-bold">S. Zhang*</span>, MTTrans: Cross-Domain Object Detection with Mean Teacher Transformer, 17th European Conference on Computer Vision (ECCV) 2022.
						<br>
						<b>Codes:</b> <a href="https://github.com/Lafite-Yu/MTTrans-OpenSource">https://github.com/Lafite-Yu/MTTrans-OpenSource</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ECCV 2022]</span>
						X. Li, J. Liu, S.Wang, C. Lyu, M. Lu, Y. Chen, A. Yao, Y. Guo, <span class="author-bold">S. Zhang*</span>, Efficient Meta-Tuning for Content-aware Neural Video Delivery, 17th European Conference on Computer Vision (ECCV) 2022.
						<br>
						<b>Codes:</b> <a href="https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022">https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[ICML 2022]</span>
						Chu X, Jin Y, Zhu W, Wang Y, Wang X, <span class="author-bold">Zhang S</span>, Mei H. DNA: Domain generalization with diversified neural averaging. In International Conference on Machine Learning (ICML) 2022 Jun 28 (pp. 4010-4034). PMLR.
						<br>
						<b>Codes:</b> <a href="https://github.com/JinYujie99/DNA">https://github.com/JinYujie99/DNA</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[IJCAI 2022]</span>
						Li T, Chen X, Dong Z, Yu W, Yan Y, Keutzer K, <span class="author-bold">Zhang S*</span>. Domain-Adaptive Text Classification with Structured Knowledge from Unlabeled Data. International Joint Conference on Artificial Intelligence (IJCAI), 2022.
						<br>
						<b>Codes:</b> <a href="https://github.com/hikaru-nara/DASK">https://github.com/hikaru-nara/DASK</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[TMM 2022]</span>
						S. Zhou, L. Wang, <span class="author-bold">S. Zhang*</span>, Z.Wang*, W.Zhu*. “Active Gradual Domain Adaptation: Dataset and Approach”, IEEE Transactions on Multimedia (TMM), 2022.
						<br>
						<b>Codes:</b> <a href="https://github.com/LianzheWang/Active-Gradual-Domain-Adaptation-Dataset-and-Approach">https://github.com/LianzheWang/Active-Gradual-Domain-Adaptation-Dataset-and-Approach</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[CVPR 2022]</span>
						C. Zhang#, M. Zhang#, <span class="author-bold">S. Zhang#</span>, et al. "Delving deep into the generalization of vision transformers under distribution shifts.", Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
						<br>
						<b>Codes:</b> <a href="https://github.com/Phoenix1153/ViT_OOD_generalization">https://github.com/Phoenix1153/ViT_OOD_generalization</a>
					</p>
					<div class="contline"></div>
					
					<p class="contc1">
						<span class="conference">[WACV 2022]</span>
						Reed CJ, Yue X, Nrusimha A, Ebrahimi S, Vijaykumar V, Mao R, Li B, <span class="author-bold">Zhang S</span>, Guillory D, Metzger S, Keutzer K. Self-supervised pretraining improves self-supervised pretraining. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2022 (pp. 2584-2594).
						<br>
						<b>Codes:</b> <a href="https://github.com/cjrd/self-supervised-pretraining?tab=readme-ov-file">https://github.com/cjrd/self-supervised-pretraining?tab=readme-ov-file</a>
					</p>
					<div class="contline"></div>


                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
  <!-- content over -->
  <table width="100%" bgcolor="#000" border="0" cellspacing="0" cellpadding="0">
    <tbody>
      <tr>
        <td align="center">
          <table width="1200px" border="0" cellspacing="0" cellpadding="0">
            <tbody>
              <tr>
                <td height="50px">
                  <div class="bottom">© Copyright 2023 HMI Lab - All rights reserved</div>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>



</body>

</html>